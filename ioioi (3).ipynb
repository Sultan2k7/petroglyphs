{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12336929,"sourceType":"datasetVersion","datasetId":7777054},{"sourceId":12337330,"sourceType":"datasetVersion","datasetId":7777375},{"sourceId":12339228,"sourceType":"datasetVersion","datasetId":7778667},{"sourceId":12347330,"sourceType":"datasetVersion","datasetId":7783976},{"sourceId":12347897,"sourceType":"datasetVersion","datasetId":7784374},{"sourceId":12347978,"sourceType":"datasetVersion","datasetId":7784433}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from ultralytics import YOLO\nimport torch\n\nmodel = YOLO('./best.pt')\nmodel2 = YOLO('./best(1).pt')\n\n# Run inference\n# results1 = model('/kaggle/input/uts-hackathon/Dataset/image_2024-10-26_17-37-16.png')[0]\n# results2 = model2('/kaggle/input/uts-hackathon/Dataset/image_2024-10-26_17-37-16.png')[0]\n\n# # Extract predictions\n# boxes1 = results1.boxes.xyxy  # or .xywh\n# scores1 = results1.boxes.conf\n# labels1 = results1.boxes.cls\n\n# boxes2 = results2.boxes.xyxy\n# scores2 = results2.boxes.conf\n# labels2 = results2.boxes.cls\n\n# # Combine predictions\n# combined_boxes = torch.cat([boxes1, boxes2], dim=0)\n# combined_scores = torch.cat([scores1, scores2], dim=0)\n# combined_labels = torch.cat([labels1, labels2], dim=0)\n\n# # Optionally put into one result-like dict\n# combined_preds = {\n#     \"boxes\": combined_boxes,\n#     \"scores\": combined_scores,\n#     \"labels\": combined_labels\n# }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T07:04:36.587011Z","iopub.execute_input":"2025-07-02T07:04:36.587372Z","iopub.status.idle":"2025-07-02T07:04:38.853450Z","shell.execute_reply.started":"2025-07-02T07:04:36.587344Z","shell.execute_reply":"2025-07-02T07:04:38.852100Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# import cv2\n# import matplotlib.pyplot as plt\n\n# image = cv2.imread('/kaggle/input/uts-hackathon/Dataset/image_2024-10-26_17-37-16.png')\n# for box, label, score in zip(combined_boxes, combined_labels, combined_scores):\n#     x1, y1, x2, y2 = map(int, box)\n#     cv2.rectangle(image, (x1, y1), (x2, y2), (0,255,0), 2)\n#     cv2.putText(image, f'{int(label)} {score:.2f}', (x1, y1 - 5),\n#                 cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,0), 1)\n\n# plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n# plt.axis('off')\n# plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T07:04:38.855430Z","iopub.execute_input":"2025-07-02T07:04:38.855876Z","iopub.status.idle":"2025-07-02T07:04:38.859954Z","shell.execute_reply.started":"2025-07-02T07:04:38.855848Z","shell.execute_reply":"2025-07-02T07:04:38.859094Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import os\nimport cv2\nimport torch\nimport numpy as np\nfrom PIL import Image, ImageEnhance\nimport torchvision.transforms as T\nimport torchvision.transforms.functional as TF\nimport matplotlib.pyplot as plt\nfrom ultralytics import YOLO\n\n# Inputs\nimage_paths = [os.path.join('./dataset',i) for i in os.listdir('./dataset')] # Replace with your list\nIMG_SIZE = 256\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# Models\n# These must be defined earlier\n# model = YOLO('model.pt')\n# model2 = YOLO('model2.pt')\n\n# Create output directories\nos.makedirs(\"output_bw\", exist_ok=True)\nos.makedirs(\"output_color\", exist_ok=True)\n\n# Main loop\nfor img_path in image_paths:\n    print(f\"Processing {img_path}...\")\n\n    img_pil = Image.open(img_path).convert(\"RGB\")\n    img_np = np.array(img_pil)\n\n    # Run inference with both models\n    results1 = model(img_path)[0]\n    results2 = model2(img_path)[0]\n\n    # Combine detections\n    boxes1 = results1.boxes.xyxy if results1.boxes is not None else torch.empty((0, 4))\n    boxes2 = results2.boxes.xyxy if results2.boxes is not None else torch.empty((0, 4))\n    boxes = torch.cat([boxes1, boxes2], dim=0)\n\n    # Init masks\n    segmented_result = np.zeros((img_np.shape[0], img_np.shape[1]), dtype=np.uint8)       # BW\n    segmented_color_result = np.zeros_like(img_np)                                        # RGB\n\n    for i, bbox in enumerate(boxes):\n        x1, y1, x2, y2 = map(int, bbox)\n\n        # 1️⃣ Crop and enhance\n        crop_img_np_rgb = img_np[y1:y2, x1:x2]\n        pil_image = Image.fromarray(crop_img_np_rgb)\n        enhanced_image = ImageEnhance.Color(pil_image).enhance(2.0)\n\n        crop_pil_gray = enhanced_image.convert(\"L\")\n        crop_img_gray = np.array(crop_pil_gray)\n\n        # 2️⃣ Threshold\n        _, thresholded_img = cv2.threshold(crop_img_gray, 140, 255, cv2.THRESH_BINARY)\n\n        # 2.5 Connected components cleanup\n        num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(thresholded_img, connectivity=8)\n        min_area = 10\n        cleaned_mask = np.zeros_like(thresholded_img)\n        for label in range(1, num_labels):\n            if stats[label, cv2.CC_STAT_AREA] >= min_area:\n                cleaned_mask[labels == label] = 255\n\n        # 3️⃣ Add to global masks\n        segmented_result[y1:y2, x1:x2] = np.maximum(segmented_result[y1:y2, x1:x2], cleaned_mask)\n\n        # Color petroglyph only\n        mask_bool = cleaned_mask.astype(bool)\n        segmented_color_result[y1:y2, x1:x2][mask_bool] = crop_img_np_rgb[mask_bool]\n\n        # (Optional) Model inference on the cropped area:\n        # crop_tensor = T.ToTensor()(crop_pil_gray)\n        # resized_crop = TF.resize(crop_tensor, [IMG_SIZE, IMG_SIZE])\n        # input_batch = resized_crop.unsqueeze(0).to(DEVICE)\n        # pred = model(input_batch)  # optional\n\n    # Save masks\n    base_name = os.path.splitext(os.path.basename(img_path))[0]\n    cv2.imwrite(f\"output_bw/{base_name}.png\", segmented_result)\n    cv2.imwrite(f\"output_color/{base_name}.png\", cv2.cvtColor(segmented_color_result, cv2.COLOR_RGB2BGR))\n\n    print(f\"Saved masks for {base_name}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T07:04:38.860589Z","iopub.execute_input":"2025-07-02T07:04:38.860842Z","iopub.status.idle":"2025-07-02T07:04:43.856691Z","shell.execute_reply.started":"2025-07-02T07:04:38.860820Z","shell.execute_reply":"2025-07-02T07:04:43.855698Z"}},"outputs":[{"name":"stdout","text":"Processing /kaggle/input/uts-hackathon/Dataset/image_2024-10-26_17-39-40(2).png...\n\nimage 1/1 /kaggle/input/uts-hackathon/Dataset/image_2024-10-26_17-39-40(2).png: 576x832 2 petroglyphs, 40.5ms\nSpeed: 6.0ms preprocess, 40.5ms inference, 167.1ms postprocess per image at shape (1, 3, 576, 832)\n\nimage 1/1 /kaggle/input/uts-hackathon/Dataset/image_2024-10-26_17-39-40(2).png: 576x832 3 petroglyphs, 9.6ms\nSpeed: 3.2ms preprocess, 9.6ms inference, 1.5ms postprocess per image at shape (1, 3, 576, 832)\nSaved masks for image_2024-10-26_17-39-40(2)\nProcessing /kaggle/input/uts-hackathon/Dataset/image_2024-10-26_17-37-17(2).png...\n\nimage 1/1 /kaggle/input/uts-hackathon/Dataset/image_2024-10-26_17-37-17(2).png: 576x832 11 petroglyphs, 9.0ms\nSpeed: 2.9ms preprocess, 9.0ms inference, 2.0ms postprocess per image at shape (1, 3, 576, 832)\n\nimage 1/1 /kaggle/input/uts-hackathon/Dataset/image_2024-10-26_17-37-17(2).png: 576x832 2 petroglyphs, 8.9ms\nSpeed: 2.7ms preprocess, 8.9ms inference, 1.5ms postprocess per image at shape (1, 3, 576, 832)\nSaved masks for image_2024-10-26_17-37-17(2)\nProcessing /kaggle/input/uts-hackathon/Dataset/image_2024-10-26_17-39-37(3).png...\n\nimage 1/1 /kaggle/input/uts-hackathon/Dataset/image_2024-10-26_17-39-37(3).png: 640x832 1 petroglyph, 40.8ms\nSpeed: 3.4ms preprocess, 40.8ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 832)\n\nimage 1/1 /kaggle/input/uts-hackathon/Dataset/image_2024-10-26_17-39-37(3).png: 640x832 3 petroglyphs, 10.0ms\nSpeed: 3.2ms preprocess, 10.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 832)\nSaved masks for image_2024-10-26_17-39-37(3)\nProcessing /kaggle/input/uts-hackathon/Dataset/image_2024-10-26_17-37-16(4).png...\n\nimage 1/1 /kaggle/input/uts-hackathon/Dataset/image_2024-10-26_17-37-16(4).png: 576x832 7 petroglyphs, 9.9ms\nSpeed: 2.9ms preprocess, 9.9ms inference, 1.5ms postprocess per image at shape (1, 3, 576, 832)\n\nimage 1/1 /kaggle/input/uts-hackathon/Dataset/image_2024-10-26_17-37-16(4).png: 576x832 13 petroglyphs, 9.7ms\nSpeed: 2.7ms preprocess, 9.7ms inference, 1.7ms postprocess per image at shape (1, 3, 576, 832)\nSaved masks for image_2024-10-26_17-37-16(4)\nProcessing /kaggle/input/uts-hackathon/Dataset/image_2024-10-26_17-39-40(1).png...\n\nimage 1/1 /kaggle/input/uts-hackathon/Dataset/image_2024-10-26_17-39-40(1).png: 576x832 2 petroglyphs, 9.0ms\nSpeed: 3.8ms preprocess, 9.0ms inference, 1.7ms postprocess per image at shape (1, 3, 576, 832)\n\nimage 1/1 /kaggle/input/uts-hackathon/Dataset/image_2024-10-26_17-39-40(1).png: 576x832 3 petroglyphs, 8.9ms\nSpeed: 3.5ms preprocess, 8.9ms inference, 1.6ms postprocess per image at shape (1, 3, 576, 832)\nSaved masks for image_2024-10-26_17-39-40(1)\nProcessing /kaggle/input/uts-hackathon/Dataset/photo_5253772797728121088_x.jpeg...\n\nimage 1/1 /kaggle/input/uts-hackathon/Dataset/photo_5253772797728121088_x.jpeg: 832x512 4 petroglyphs, 44.5ms\nSpeed: 2.5ms preprocess, 44.5ms inference, 1.7ms postprocess per image at shape (1, 3, 832, 512)\n\nimage 1/1 /kaggle/input/uts-hackathon/Dataset/photo_5253772797728121088_x.jpeg: 832x512 2 petroglyphs, 8.7ms\nSpeed: 2.2ms preprocess, 8.7ms inference, 1.3ms postprocess per image at shape (1, 3, 832, 512)\nSaved masks for photo_5253772797728121088_x\nProcessing /kaggle/input/uts-hackathon/Dataset/image_2024-10-26_17-37-16(9).png...\n\nimage 1/1 /kaggle/input/uts-hackathon/Dataset/image_2024-10-26_17-37-16(9).png: 576x832 12 petroglyphs, 9.7ms\nSpeed: 2.8ms preprocess, 9.7ms inference, 1.5ms postprocess per image at shape (1, 3, 576, 832)\n\nimage 1/1 /kaggle/input/uts-hackathon/Dataset/image_2024-10-26_17-37-16(9).png: 576x832 6 petroglyphs, 9.6ms\nSpeed: 2.7ms preprocess, 9.6ms inference, 1.4ms postprocess per image at shape (1, 3, 576, 832)\nSaved masks for image_2024-10-26_17-37-16(9)\nProcessing /kaggle/input/uts-hackathon/Dataset/image_2024-10-26_17-39-36(1).png...\n\nimage 1/1 /kaggle/input/uts-hackathon/Dataset/image_2024-10-26_17-39-36(1).png: 576x832 3 petroglyphs, 9.0ms\nSpeed: 2.9ms preprocess, 9.0ms inference, 1.5ms postprocess per image at shape (1, 3, 576, 832)\n\nimage 1/1 /kaggle/input/uts-hackathon/Dataset/image_2024-10-26_17-39-36(1).png: 576x832 (no detections), 8.9ms\nSpeed: 2.8ms preprocess, 8.9ms inference, 0.7ms postprocess per image at shape (1, 3, 576, 832)\nSaved masks for image_2024-10-26_17-39-36(1)\nProcessing /kaggle/input/uts-hackathon/Dataset/photo_5253772797728121093_x.jpeg...\n\nimage 1/1 /kaggle/input/uts-hackathon/Dataset/photo_5253772797728121093_x.jpeg: 832x704 (no detections), 39.7ms\nSpeed: 3.3ms preprocess, 39.7ms inference, 0.7ms postprocess per image at shape (1, 3, 832, 704)\n\nimage 1/1 /kaggle/input/uts-hackathon/Dataset/photo_5253772797728121093_x.jpeg: 832x704 (no detections), 10.5ms\nSpeed: 3.4ms preprocess, 10.5ms inference, 0.7ms postprocess per image at shape (1, 3, 832, 704)\nSaved masks for photo_5253772797728121093_x\nProcessing /kaggle/input/uts-hackathon/Dataset/image_2024-10-26_17-39-33.png...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_708/2371052844.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mimg_pil\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mimg_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_pil\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# Run inference with both models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":3},{"cell_type":"code","source":"import zipfile\nimport os\n\ndef zip_folders(zip_name, folders):\n    with zipfile.ZipFile(zip_name, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        for folder in folders:\n            for root, _, files in os.walk(folder):\n                for file in files:\n                    file_path = os.path.join(root, file)\n                    arcname = os.path.relpath(file_path, start=os.path.dirname(folder))\n                    zipf.write(file_path, arcname)\n\n# Usage\nzip_folders('masks_output.zip', ['output_bw', 'output_color'])\nprint(\"✅ Created masks_output.zip with both folders.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T07:04:43.857172Z","iopub.status.idle":"2025-07-02T07:04:43.857403Z","shell.execute_reply.started":"2025-07-02T07:04:43.857279Z","shell.execute_reply":"2025-07-02T07:04:43.857289Z"}},"outputs":[],"execution_count":null}]}